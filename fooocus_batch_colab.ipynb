{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé® Fooocus Batch Generator - Wildcard M√≥dszer\n",
                "\n",
                "Ez a notebook **automatikusan gener√°lja** a k√©peket TXT f√°jlokb√≥l **wildcard** haszn√°lat√°val.\n",
                "\n",
                "## üìã M≈±k√∂d√©s\n",
                "\n",
                "1. TXT f√°jlokat **wildcard** f√°jlokk√° alak√≠tja\n",
                "2. Fooocus-t elind√≠tja\n",
                "3. **Gradio API**-n kereszt√ºl gener√°lja a k√©peket\n",
                "4. Automatikusan let√∂lti az eredm√©nyeket\n",
                "\n",
                "## üöÄ Haszn√°lat\n",
                "\n",
                "Futtasd sorban az √∂sszes cell√°t!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Fooocus Telep√≠t√©se"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Fooocus kl√≥noz√°sa\n",
                "if not os.path.exists(\"/content/Fooocus\"):\n",
                "    print(\"üì• Fooocus let√∂lt√©se...\")\n",
                "    !git clone https://github.com/lllyasviel/Fooocus.git\n",
                "\n",
                "# F√ºgg≈ës√©gek\n",
                "!apt-get -y install -qq aria2\n",
                "!pip install pygit2==1.15.1 gradio_client==0.8.1 -q\n",
                "\n",
                "print(\"‚úì Telep√≠t√©s k√©sz!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ TXT F√°jlok Felt√∂lt√©se"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import shutil\n",
                "\n",
                "# Prompts mappa\n",
                "prompts_dir = \"/content/prompts\"\n",
                "os.makedirs(prompts_dir, exist_ok=True)\n",
                "\n",
                "print(\"üì§ T√∂ltsd fel a TXT f√°jljaidat:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# F√°jlok √°thelyez√©se\n",
                "for filename in uploaded.keys():\n",
                "    if filename.endswith('.txt'):\n",
                "        shutil.move(filename, f\"{prompts_dir}/{filename}\")\n",
                "        print(f\"‚úì {filename}\")\n",
                "\n",
                "print(\"\\n‚úì Felt√∂lt√©s k√©sz!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Wildcard F√°jlok L√©trehoz√°sa"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "# Wildcard mappa\n",
                "wildcard_dir = \"/content/Fooocus/wildcards\"\n",
                "os.makedirs(wildcard_dir, exist_ok=True)\n",
                "\n",
                "# TXT -> Wildcard konverzi√≥\n",
                "txt_files = list(Path(prompts_dir).glob(\"*.txt\"))\n",
                "\n",
                "print(f\"üìù {len(txt_files)} f√°jl konvert√°l√°sa wildcard-okk√°...\\n\")\n",
                "\n",
                "wildcard_mapping = {}\n",
                "\n",
                "for txt_file in txt_files:\n",
                "    # Wildcard n√©v (f√°jln√©v .txt n√©lk√ºl)\n",
                "    wildcard_name = txt_file.stem\n",
                "    wildcard_path = f\"{wildcard_dir}/{wildcard_name}.txt\"\n",
                "    \n",
                "    # M√°sol√°s\n",
                "    shutil.copy(txt_file, wildcard_path)\n",
                "    \n",
                "    # Promptok sz√°ma\n",
                "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
                "        prompts = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
                "    \n",
                "    wildcard_mapping[wildcard_name] = len(prompts)\n",
                "    print(f\"‚úì __{wildcard_name}__ ‚Üí {len(prompts)} prompt\")\n",
                "\n",
                "print(f\"\\n‚úì {len(wildcard_mapping)} wildcard l√©trehozva!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Be√°ll√≠t√°sok"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# ========== BE√ÅLL√çT√ÅSOK ==========\n",
                "\n",
                "PERFORMANCE = \"Extreme Speed\"  # Speed, Quality, Extreme Speed\n",
                "ASPECT_RATIO = \"1344*704\"      # 1344*704, 1152*896, stb.\n",
                "STYLES = [\"SAI Comic Book\"]    # St√≠lusok\n",
                "MODEL = \"juggernautXL_v8Rundiffusion.safetensors\"\n",
                "OUTPUT_FORMAT = \"png\"\n",
                "\n",
                "# Preset ment√©se\n",
                "preset_path = \"/content/Fooocus/presets/batch_auto.json\"\n",
                "\n",
                "settings = {\n",
                "    \"default_performance\": PERFORMANCE,\n",
                "    \"default_aspect_ratio\": ASPECT_RATIO,\n",
                "    \"default_styles\": STYLES,\n",
                "    \"default_model\": MODEL,\n",
                "    \"default_output_format\": OUTPUT_FORMAT\n",
                "}\n",
                "\n",
                "with open(preset_path, 'w') as f:\n",
                "    json.dump(settings, f, indent=4)\n",
                "\n",
                "print(\"‚úì Be√°ll√≠t√°sok:\")\n",
                "print(f\"  Performance: {PERFORMANCE}\")\n",
                "print(f\"  Felbont√°s: {ASPECT_RATIO}\")\n",
                "print(f\"  St√≠lus: {STYLES}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Fooocus Ind√≠t√°sa (h√°tt√©rben)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import time\n",
                "import requests\n",
                "\n",
                "os.chdir(\"/content/Fooocus\")\n",
                "\n",
                "print(\"üöÄ Fooocus ind√≠t√°sa...\")\n",
                "\n",
                "# H√°tt√©rben ind√≠t√°s\n",
                "process = subprocess.Popen(\n",
                "    ['python', 'entry_with_update.py', '--share', '--always-high-vram', '--preset', 'batch_auto'],\n",
                "    stdout=subprocess.PIPE,\n",
                "    stderr=subprocess.PIPE\n",
                ")\n",
                "\n",
                "# V√°rakoz√°s, am√≠g a szerver elindul\n",
                "print(\"‚è≥ V√°rakoz√°s a szerver indul√°s√°ra...\")\n",
                "max_wait = 300  # 5 perc max\n",
                "start_time = time.time()\n",
                "server_ready = False\n",
                "\n",
                "while time.time() - start_time < max_wait:\n",
                "    try:\n",
                "        response = requests.get(\"http://127.0.0.1:7865\", timeout=2)\n",
                "        if response.status_code == 200:\n",
                "            server_ready = True\n",
                "            break\n",
                "    except:\n",
                "        pass\n",
                "    \n",
                "    elapsed = int(time.time() - start_time)\n",
                "    print(f\"  V√°rakoz√°s... ({elapsed}s)\", end='\\r')\n",
                "    time.sleep(5)\n",
                "\n",
                "if server_ready:\n",
                "    print(f\"\\n‚úì Fooocus elindult! ({int(time.time() - start_time)}s)\")\n",
                "else:\n",
                "    print(\"\\n‚ùå Timeout! A szerver nem indult el.\")\n",
                "    print(\"Pr√≥b√°ld √∫jra futtatni ezt a cell√°t.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ AUTOMATIKUS BATCH GENER√ÅL√ÅS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gradio_client import Client\n",
                "import time\n",
                "\n",
                "# Gradio client\n",
                "print(\"üîå Csatlakoz√°s Fooocus-hoz...\")\n",
                "\n",
                "try:\n",
                "    client = Client(\"http://127.0.0.1:7865\")\n",
                "    print(\"‚úì Csatlakozva!\\n\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Csatlakoz√°si hiba: {e}\")\n",
                "    print(\"\\nEllen≈ërizd, hogy az 5. cella sikeresen lefutott-e!\")\n",
                "    print(\"Ha nem, futtasd √∫jra az 5. cell√°t.\")\n",
                "    raise\n",
                "\n",
                "# ========== GENER√ÅL√ÅS ==========\n",
                "\n",
                "total_generated = 0\n",
                "\n",
                "for wildcard_name, prompt_count in wildcard_mapping.items():\n",
                "    print(f\"{'='*60}\")\n",
                "    print(f\"üé® Wildcard: __{wildcard_name}__\")\n",
                "    print(f\"{'='*60}\")\n",
                "    print(f\"Promptok: {prompt_count}\\n\")\n",
                "    \n",
                "    # Batch-enk√©nt gener√°l√°s (max 32)\n",
                "    batch_size = 32\n",
                "    batches = (prompt_count + batch_size - 1) // batch_size\n",
                "    \n",
                "    for batch_num in range(batches):\n",
                "        start_idx = batch_num * batch_size\n",
                "        end_idx = min(start_idx + batch_size, prompt_count)\n",
                "        current_batch_size = end_idx - start_idx\n",
                "        \n",
                "        print(f\"\\nüì¶ Batch {batch_num + 1}/{batches} ({current_batch_size} k√©p)\")\n",
                "        \n",
                "        # Prompt wildcard-dal\n",
                "        prompt = f\"__{wildcard_name}__\"\n",
                "        \n",
                "        try:\n",
                "            # Gradio API h√≠v√°s\n",
                "            result = client.predict(\n",
                "                prompt=prompt,\n",
                "                negative_prompt=\"\",\n",
                "                style_selections=STYLES,\n",
                "                performance_selection=PERFORMANCE,\n",
                "                aspect_ratios_selection=ASPECT_RATIO,\n",
                "                image_number=current_batch_size,\n",
                "                output_format=OUTPUT_FORMAT,\n",
                "                image_seed=-1,\n",
                "                seed_random=True,\n",
                "                api_name=\"/generate\"\n",
                "            )\n",
                "            \n",
                "            total_generated += current_batch_size\n",
                "            print(f\"  ‚úì {current_batch_size} k√©p gener√°lva\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"  ‚úó Hiba: {e}\")\n",
                "        \n",
                "        # Kis sz√ºnet batch-ek k√∂z√∂tt\n",
                "        if batch_num < batches - 1:\n",
                "            time.sleep(2)\n",
                "    \n",
                "    print(f\"\\n‚úì __{wildcard_name}__ k√©sz!\\n\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üéâ GENER√ÅL√ÅS BEFEJEZVE!\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"√ñsszesen: {total_generated} k√©p\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ K√©pek Let√∂lt√©se"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from pathlib import Path\n",
                "\n",
                "output_dir = \"/content/Fooocus/outputs\"\n",
                "\n",
                "if os.path.exists(output_dir):\n",
                "    print(\"üì¶ T√∂m√∂r√≠t√©s...\")\n",
                "    shutil.make_archive('/content/generated_images', 'zip', output_dir)\n",
                "    \n",
                "    # Statisztika\n",
                "    images = list(Path(output_dir).glob('**/*.png')) + list(Path(output_dir).glob('**/*.jpg'))\n",
                "    total_size = sum(img.stat().st_size for img in images)\n",
                "    \n",
                "    print(f\"\\nüìä Statisztika:\")\n",
                "    print(f\"  K√©pek: {len(images)}\")\n",
                "    print(f\"  M√©ret: {total_size / 1024 / 1024:.2f} MB\")\n",
                "    \n",
                "    print(\"\\n‚¨áÔ∏è Let√∂lt√©s...\")\n",
                "    files.download('/content/generated_images.zip')\n",
                "    \n",
                "    print(\"\\n‚úì K√©sz!\")\n",
                "else:\n",
                "    print(\"‚ùå Nincs k√©p!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üí° Hogyan M≈±k√∂dik?\n",
                "\n",
                "1. **TXT f√°jlok** ‚Üí **Wildcard f√°jlok** (`/content/Fooocus/wildcards/`)\n",
                "2. **Fooocus ind√≠t√°sa** h√°tt√©rben Gradio interface-szel\n",
                "3. **Szerver ellen≈ërz√©s** - V√°r, am√≠g a szerver elindul (max 5 perc)\n",
                "4. **Gradio API** haszn√°lata a gener√°l√°shoz\n",
                "5. Prompt: `__wildcard_n√©v__` ‚Üí Fooocus random v√°laszt a list√°b√≥l\n",
                "6. **Batch-enk√©nt** (max 32) gener√°l√°s\n",
                "\n",
                "## üéØ El≈ëny√∂k\n",
                "\n",
                "- ‚úÖ **M≈±k√∂dik** - Nincs args_manager konfliktus\n",
                "- ‚úÖ **Automatikus** - Gradio API-n kereszt√ºl\n",
                "- ‚úÖ **Batch-elt** - 32 k√©p egyszerre\n",
                "- ‚úÖ **Testreszabhat√≥** - Performance, st√≠lus, felbont√°s\n",
                "- ‚úÖ **Megb√≠zhat√≥** - Ellen≈ërzi, hogy a szerver elindult-e\n",
                "\n",
                "## üêõ Hibaelh√°r√≠t√°s\n",
                "\n",
                "**\"Connection refused\" hiba:**\n",
                "- Futtasd √∫jra az 5. cell√°t (Fooocus ind√≠t√°sa)\n",
                "- V√°rj, am√≠g megjelenik a \"‚úì Fooocus elindult!\" √ºzenet\n",
                "\n",
                "**\"Timeout\" hiba:**\n",
                "- A Colab GPU lass√∫ lehet\n",
                "- Pr√≥b√°ld √∫jra futtatni az 5. cell√°t\n",
                "- Ellen≈ërizd a Runtime ‚Üí Change runtime type ‚Üí GPU be√°ll√≠t√°st\n",
                "\n",
                "---\n",
                "\n",
                "**GitHub**: [dwick90/Fooocus_txt_prompts](https://github.com/dwick90/Fooocus_txt_prompts)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}